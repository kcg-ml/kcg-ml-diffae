# DiffAE: Diffusion Autoencoder

This document outlines the key workflows for working with the DiffAE (Diffusion Autoencoder) models.

## Table of Contents
- [Installation and Setup](#1-installation-and-setup)
- [Training Workflows](#2-training-workflows)
- [Inference Workflows](#3-inference-workflows)
- [Advanced Workflows](#4-advanced-workflows)
- [Evaluation Workflows](#5-evaluation-workflows)
- [TensorBoard Visualization](#6-tensorboard-visualization)

## 1. Installation and Setup

### Standard Installation
1. Create a Python virtual environment:
    ```bash
    python -m venv venv
    ```

2. Activate the environment:
    ```bash
    # On Windows
    .\venv\Scripts\activate

    # On Linux/macOS
    source ./venv/bin/activate
    ```

3. Install the dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### CUDA-Specific Installation
If you need to match a specific CUDA version:
```bash
# Replace cu118 with your CUDA version, e.g., cu117, cu116
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

## 2. Training Workflows

### Training FFHQ256 Model
For high-resolution FFHQ 256x256 images:
```bash
sbatch run_ffhq256.sh
```

Use the multi-GPU, multi-node script:
```python
from templates import ffhq256_autoenc
from experiment import train

conf = ffhq256_autoenc()
train(conf, gpus=[0, 1, 2, 3], nodes=2)
```

### Training Other Models
For other datasets:
```python
# FFHQ 128x128
from templates import ffhq128_autoenc
conf = ffhq128_autoenc()
train(conf, gpus=[0])

# CelebA 64x64
from templates import celeba64_autoenc
conf = celeba64_autoenc()
train(conf, gpus=[0])
```

## 3. Inference Workflows

### Computing Semantic Representation (xsem)
Extract the semantic encoding from an image:
```python
from PIL import Image
import torch
import torchvision.transforms as transforms
from templates import ffhq256_autoenc
from experiment import LitModel

# Setup
device = 'cuda' if torch.cuda.is_available() else 'cpu'
conf = ffhq256_autoenc()
model = LitModel(conf)
model.to(device)

# Load and preprocess image
img = Image.open('input_image.jpg').resize((256, 256)).convert('RGB')
transform = transforms.ToTensor()
x = transform(img).unsqueeze(0).to(device)

# Compute semantic encoding
with torch.no_grad():
     xsem = model.encode(x)
```

### Generating Noised Image Representation (xt)
Create a noised representation of an image at timestep t:
```python
# Continuing from above
timestep = 500
cond = xsem

with torch.no_grad():
     xt = model.encode_stochastic(x, cond, T=timestep)
```

### Image Generation from xsem and xt
Decode an image using semantic encoding and a noised image:
```python
# Using the computed semantic encoding (xsem) and random noise (random_x_t)
with torch.no_grad():
     generated_image = model.decode(xsem, random_x_t)
     
# Convert to PIL image for visualization
from torchvision.transforms.functional import to_pil_image
pil_image = to_pil_image(generated_image[0].cpu().clamp(-1, 1) * 0.5 + 0.5)
pil_image.save('generated_image.png')
```

### Unconditional Sampling
Generate completely new images:
```python
# Sample N images
N = 4
with torch.no_grad():
     samples = model.sample(N, device=device)
     
# Save samples
from torchvision.utils import save_image
save_image(samples.cpu() * 0.5 + 0.5, 'samples.png', nrow=2)
```

## 4. Advanced Workflows

### Inference on Entire Dataset
Process and evaluate an entire dataset:
```python
# Set up dataset
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder

dataset = ImageFolder('path/to/images', transform=transform)
dataloader = DataLoader(dataset, batch_size=16, shuffle=False)

# Run inference
results = []
for batch in dataloader:
     imgs, _ = batch
     imgs = imgs.to(device)
     with torch.no_grad():
          xsem = model.encode(imgs)
          # Process further as needed
          results.append(xsem)
```

### Style Mixing
Mix semantic encodings from different images:
```python
# Get semantic encodings for two images
img1 = transform(Image.open('image1.jpg').resize((256, 256)).convert('RGB')).unsqueeze(0).to(device)
img2 = transform(Image.open('image2.jpg').resize((256, 256)).convert('RGB')).unsqueeze(0).to(device)

with torch.no_grad():
     xsem1 = model.encode(img1)
     xsem2 = model.encode(img2)
     
     # Mix the semantic encodings (e.g., 70% from image1, 30% from image2)
     mixed_xsem = 0.7 * xsem1 + 0.3 * xsem2
     
     # Generate random noise
     random_x_t = torch.randn_like(img1)
     
     # Generate image with mixed semantics
     mixed_image = model.decode(mixed_xsem, random_x_t)
```

### Progressive Denoising Visualization
Visualize the denoising process from a noised image:
```python
import matplotlib.pyplot as plt

# Start with random noise
x_t = torch.randn_like(x)
timesteps = [800, 600, 400, 200, 0]

images = []

with torch.no_grad():
     for t in timesteps:
          # Convert timestep to tensor
          t_tensor = torch.tensor([t], device=device).long()
          
          # Predict denoised image at this timestep
          pred = model(x_t=x_t, xsem=xsem, t=t_tensor)
          images.append(pred[0].cpu().clamp(-1, 1) * 0.5 + 0.5)

# Visualize
fig, axes = plt.subplots(1, len(images), figsize=(15, 3))
for i, img in enumerate(images):
     axes[i].imshow(img.permute(1, 2, 0))
     axes[i].set_title(f"t={timesteps[i]}")
     axes[i].axis('off')
plt.savefig('denoising_process.png')
```

## 5. Evaluation Workflows

### Computing FID Score
Calculate the Fr√©chet Inception Distance:
```python
from metrics import compute_fid

# Generate samples
samples = []
for i in range(10):  # Generate batches of samples
     with torch.no_grad():
          batch_samples = model.sample(16, device=device)
     samples.append(batch_samples)
samples = torch.cat(samples, dim=0)  # Concatenate all batches

# Calculate FID
fid_score = compute_fid(samples, 'path/to/reference/stats.npz')
print(f"FID Score: {fid_score}")
```

### Model Checkpoint Management
Save and load model checkpoints:
```python
# Save model
checkpoint = {
     'state_dict': model.state_dict(),
     'config': conf.as_dict_jsonable()
}
torch.save(checkpoint, 'diffae_model.ckpt')

# Load model
checkpoint = torch.load('diffae_model.ckpt')
conf = ffhq256_autoenc()  # Create default config
model = LitModel(conf)
model.load_state_dict(checkpoint['state_dict'])
```

## 6. TensorBoard Visualization
View training progress and generated samples:
```bash
tensorboard --logdir=path/to/logdir
```

Key metrics to observe:
- Training loss
- Learning rate
- Sample images
- FID scores (if logged)